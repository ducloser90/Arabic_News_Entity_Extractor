{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Run this if you are using colab\n",
        "!pip install -qU fsspec==2025.3.0"
      ],
      "metadata": {
        "id": "LhnzIXVOEqFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU transformers==4.48.3 datasets==3.2.0 peft accelerate bitsandbytes\n",
        "!pip install -qU json-repair"
      ],
      "metadata": {
        "id": "VzUBabRyE0Kv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import wandb\n",
        "\n",
        "hf_token = userdata.get('huggingface')\n",
        "!huggingface-cli login --token {hf_token}"
      ],
      "metadata": {
        "id": "IF4xAeOyE6SX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "from os.path import join\n",
        "import random\n",
        "from tqdm.auto import tqdm\n",
        "import requests\n",
        "\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import List, Optional, Literal\n",
        "from datetime import datetime\n",
        "\n",
        "import json_repair\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "data_dir = \"/content/drive/MyDrive/llm-finetuning\"\n",
        "base_model_id = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
        "\n",
        "device = \"cuda\"\n",
        "torch_dtype = None\n",
        "\n",
        "def parse_json(text):\n",
        "    try:\n",
        "        return json_repair.loads(text)\n",
        "    except:\n",
        "        return None"
      ],
      "metadata": {
        "id": "YQ_nsgptIoyZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "story = \"\"\"\n",
        "هل يمكننا التمييز بين ما إذا كنا نتحدث إلى إنسان آخر أم إلى ذكاء اصطناعي؟ لطالما كان هذا أحد الأسئلة التي يطرحها الناس عند تقييم مدى ذكاء الحواسيب فعلياً.\n",
        "\n",
        "ويعود أصل هذا التساؤل إلى اختبار تورينغ، الذي وضعه عالم الرياضيات وعلوم الحاسوب الإنجليزي آلان تورينغ عام 1950، محوّلاً التفكير الفلسفي حول ذكاء الآلة إلى اختبار تجريبي للمرة الأولى.\n",
        "\n",
        "وبحسب هذا الاختبار، إذا كان سلوك الحاسوب غير قابل للتمييز عن سلوك الإنسان، فإنه يُعدّ حينها مظهراً من مظاهر \"السلوك الذكي\".\n",
        "لكن عندما قيل إن روبوت محادثة يعمل بالذكاء الاصطناعي قد اجتاز الاختبار لأول مرة في عام 2014 - بدلاً من أن تكون لحظة فاصلة، فقد أثارت جدلاً واسعاً.\n",
        "لعبة تقليد\n",
        "واختبار تورينغ هو لعبة تقليد، يتواصل فيها شخص عبر النص مع إنسان آخر وحاسوب.\n",
        "\n",
        "يُسمح له بطرح أي أسئلة يشاء، قبل أن يُطلب منه في النهاية تحديد أيهما الإنسان وأيهما الآلة.\n",
        "\n",
        "وقال الدكتور كاميرون جونز، الأستاذ المساعد في علم النفس بجامعة ستوني بروك في نيويورك: \"قال تورينغ إنه إذا لم يتمكن الناس من التمييز بشكل موثوق بين البشر والآلات، فلن تكون لدينا أي أسس للقول إن الإنسان قادر على التفكير بينما الآلة غير قادرة على ذلك\".\n",
        "\n",
        "وكان تورينغ قد توقع أنه بحلول عام 2000، ستصبح الحواسيب قادرة على اجتياز هذا الاختبار والتظاهر بأنها بشر، بعد خمس دقائق من الأسئلة، في ما لا يقل عن 30 في المئة من الحالات.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "TFrYVOtmI3RG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "StoryCategory = Literal[\n",
        "    \"politics\", \"sports\", \"art\", \"technology\", \"economy\",\n",
        "    \"health\", \"entertainment\", \"science\",\n",
        "    \"not_specified\"\n",
        "]\n",
        "\n",
        "EntityType = Literal[\n",
        "    \"person-male\", \"person-female\", \"location\", \"organization\", \"event\", \"time\",\n",
        "    \"quantity\", \"money\", \"product\", \"law\", \"disease\", \"artifact\", \"not_specified\"\n",
        "]\n",
        "\n",
        "class Entity(BaseModel):\n",
        "    entity_value: str = Field(..., description=\"The actual name or value of the entity.\")\n",
        "    entity_type: EntityType = Field(..., description=\"The type of recognized entity.\")\n",
        "\n",
        "class NewsDetails(BaseModel):\n",
        "    story_title: str = Field(..., min_length=5, max_length=300,\n",
        "                             description=\"A fully informative and SEO optimized title of the story.\")\n",
        "\n",
        "    story_keywords: List[str] = Field(..., min_items=1,\n",
        "                                      description=\"Relevant keywords associated with the story.\")\n",
        "\n",
        "    story_summary: List[str] = Field(\n",
        "                                    ..., min_items=1, max_items=5,\n",
        "                                    description=\"Summarized key points about the story (1-5 points).\"\n",
        "                                )\n",
        "\n",
        "    story_category: StoryCategory = Field(..., description=\"Category of the news story.\")\n",
        "\n",
        "    story_entities: List[Entity] = Field(..., min_items=1, max_items=10,\n",
        "                                        description=\"List of identified entities in the story.\")\n"
      ],
      "metadata": {
        "id": "-g9UqHxOJn_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "details_extraction_messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"\\n\".join([\n",
        "            \"You are an NLP data paraser.\",\n",
        "            \"You will be provided by an Arabic text associated with a Pydantic scheme.\",\n",
        "            \"Generate the ouptut in the same story language.\",\n",
        "            \"You have to extract JSON details from text according the Pydantic details.\",\n",
        "            \"Extract details as mentioned in text.\",\n",
        "            \"Do not generate any introduction or conclusion.\"\n",
        "        ])\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"\\n\".join([\n",
        "            \"## Story:\",\n",
        "            story.strip(),\n",
        "            \"\",\n",
        "\n",
        "            \"## Pydantic Details:\",\n",
        "            json.dumps(\n",
        "                NewsDetails.model_json_schema(), ensure_ascii=False\n",
        "            ),\n",
        "            \"\",\n",
        "\n",
        "            \"## Story Details:\",\n",
        "            \"```json\"\n",
        "        ])\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "j4-X0E91JreY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_id,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype = torch_dtype\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_id)"
      ],
      "metadata": {
        "id": "6Au-Yng57ZZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = tokenizer.apply_chat_template(\n",
        "    details_extraction_messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True\n",
        ")\n",
        "\n",
        "model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
        "\n",
        "generated_ids = model.generate(\n",
        "    model_inputs.input_ids,\n",
        "    max_new_tokens=1024,\n",
        "    do_sample=False, top_k=None, temperature=None, top_p=None,\n",
        ")\n",
        "\n",
        "generated_ids = [\n",
        "    output_ids[len(input_ids):]\n",
        "    for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "]\n",
        "\n",
        "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
      ],
      "metadata": {
        "id": "coJEFfI98amn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "id": "vZfYD0K5-SLF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = []\n",
        "with open(\"/content/drive/MyDrive/llm-finetuning/dataset/data.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        data.append(json.loads(line))"
      ],
      "metadata": {
        "id": "xbaDeBvPNhT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sample_sz = 2000\n",
        "\n",
        "train_ds = data[:train_sample_sz]\n",
        "eval_ds = data[train_sample_sz:]\n",
        "\n",
        "os.makedirs(join(data_dir, \"datasets\", \"llamafactory-finetune-data\"), exist_ok=True)\n",
        "\n",
        "with open(join(\"/content/drive/MyDrive/llm-finetuning/dataset\", \"train.json\"), \"w\") as dest:\n",
        "    json.dump(train_ds, dest, ensure_ascii=False, default=str)\n",
        "\n",
        "with open(join(\"/content/drive/MyDrive/llm-finetuning/dataset\", \"val.json\"), \"w\", encoding=\"utf8\") as dest:\n",
        "    json.dump(eval_ds, dest, ensure_ascii=False, default=str)"
      ],
      "metadata": {
        "id": "XXsVbZxZKV_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"json\", data_files={\n",
        "    \"train\": \"/content/drive/MyDrive/llm-finetuning/dataset/train.json\",\n",
        "    \"validation\": \"/content/drive/MyDrive/llm-finetuning/dataset/val.json\"\n",
        "})\n",
        "\n",
        "def tokenize_function(sample):\n",
        "    def ensure_string(val):\n",
        "        if isinstance(val, (dict, list)):\n",
        "            return json.dumps(val, ensure_ascii=False)\n",
        "        return str(val)\n",
        "\n",
        "    task = ensure_string(sample['task'])\n",
        "    story = ensure_string(sample['story'])\n",
        "    schema = ensure_string(sample['output_scheme'])\n",
        "    response = ensure_string(sample['response'])\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": task},\n",
        "        {\"role\": \"user\", \"content\": f\"## Story:\\n{story}\\n\\n## Pydantic Details:\\n{schema}\\n\\n## Story Details:\\n```json\"},\n",
        "        {\"role\": \"assistant\", \"content\": response}\n",
        "    ]\n",
        "\n",
        "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
        "\n",
        "    tokenized = tokenizer(\n",
        "        text,\n",
        "        truncation=True,\n",
        "        max_length=1024,\n",
        "        padding=False\n",
        "    )\n",
        "\n",
        "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
        "\n",
        "    return tokenized\n",
        "\n",
        "column_names = dataset[\"train\"].column_names\n",
        "train_data = dataset[\"train\"].map(tokenize_function, remove_columns=column_names)\n",
        "val_data = dataset[\"validation\"].map(tokenize_function, remove_columns=column_names)"
      ],
      "metadata": {
        "id": "bYmUDwfjNtxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.decode(train_data[0][\"input_ids\"]))"
      ],
      "metadata": {
        "id": "qFxxt_HTP66b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_id,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    r=64,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "model = get_peft_model(model, peft_config)"
      ],
      "metadata": {
        "id": "tkAiBqbkOxn8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForSeq2Seq\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./qwen-news-finetuned\",\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=1e-4,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    num_train_epochs=3,\n",
        "    warmup_ratio=0.1,\n",
        "    logging_steps=10,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=100,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=100,\n",
        "    fp16=not torch.cuda.is_bf16_supported(),\n",
        "    bf16=torch.cuda.is_bf16_supported(),\n",
        "    optim=\"paged_adamw_32bit\",\n",
        "    report_to=None,\n",
        "    remove_unused_columns=True,\n",
        "    push_to_hub=False\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(\n",
        "    tokenizer,\n",
        "    model=model,\n",
        "    pad_to_multiple_of=8,\n",
        "    return_tensors=\"pt\",\n",
        "    padding=True\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    train_dataset=train_data,\n",
        "    eval_dataset=val_data,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "T29_5W5QO7Np"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k_C5id1UPaiP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}